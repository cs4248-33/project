{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNXjoRHhVal4"
      },
      "source": [
        "# Mount Drive to access datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Luw2VoMXFt7a",
        "outputId": "b7130f6b-a2dd-4774-dc6f-260f633b1678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import jieba"
      ],
      "metadata": {
        "id": "O1xYIrkAes-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START_TOKEN = \"<s>\"\n",
        "END_TOKEN = \"</s>\""
      ],
      "metadata": {
        "id": "Ce6OMHenevXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    def __init__(self):\n",
        "        jieba.enable_parallel(8)\n",
        "\n",
        "        self.en_context_text_processor = None\n",
        "        self.zh_context_text_processor = None\n",
        "\n",
        "        self.train_ds = None\n",
        "        self.val_ds = None\n",
        "\n",
        "    def build(self, train_data_path, val_data_path, max_vocab_size=20000):\n",
        "        train_data = pd.read_csv(train_data_path, header=0)\n",
        "        val_data = pd.read_csv(val_data_path, header=0)\n",
        "\n",
        "        BATCH_SIZE = 128\n",
        "\n",
        "        train_zh_tokenised = [\" \".join(jieba.cut(row[\"zh\"])) for _, row in train_data.iterrows()]\n",
        "        val_zh_tokenised = [\" \".join(jieba.cut(row[\"zh\"])) for _, row in val_data.iterrows()]\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((train_data['en'].tolist(), train_zh_tokenised)).shuffle(len(train_data)).batch(BATCH_SIZE)\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((val_data['en'].tolist(), val_zh_tokenised)).shuffle(len(val_data)).batch(BATCH_SIZE)\n",
        "\n",
        "        # Build vocab dictionary\n",
        "        en_context_text_processor = tf.keras.layers.TextVectorization(\n",
        "            standardize=self._preprocess_en,\n",
        "            max_tokens=max_vocab_size,\n",
        "            ragged=True)\n",
        "        zh_context_text_processor = tf.keras.layers.TextVectorization(\n",
        "            standardize=self._preprocess_zh,\n",
        "            max_tokens=max_vocab_size,\n",
        "            ragged=True)\n",
        "\n",
        "        en_context_text_processor.adapt(train_dataset.map(lambda en, _: en))\n",
        "        zh_context_text_processor.adapt(train_dataset.map(lambda _, zh: zh))\n",
        "        self.en_context_text_processor = en_context_text_processor\n",
        "        self.zh_context_text_processor = zh_context_text_processor\n",
        "\n",
        "        self.train_ds = train_dataset.map(self._process_text, tf.data.AUTOTUNE)\n",
        "        self.val_ds = val_dataset.map(self._process_text, tf.data.AUTOTUNE)\n",
        "\n",
        "    def _preprocess_en(self, en_text):\n",
        "        en_text = tf.strings.lower(en_text)\n",
        "        en_text = tf.strings.regex_replace(en_text, '[.?!,]', r' \\0 ')\n",
        "        en_text = tf.strings.join([START_TOKEN, en_text, END_TOKEN], separator=' ')\n",
        "        en_text = tf.strings.strip(en_text)\n",
        "\n",
        "        return en_text\n",
        "\n",
        "    def _preprocess_zh(self, zh_text):\n",
        "        zh_text = tf.strings.join([START_TOKEN, zh_text, END_TOKEN], separator=' ')\n",
        "        zh_text = tf.strings.strip(zh_text)\n",
        "\n",
        "        return zh_text\n",
        "\n",
        "    def _process_text(self, en_text, zh_text):\n",
        "        en_padded = self.en_context_text_processor(en_text).to_tensor()\n",
        "        zh_id_vector = self.zh_context_text_processor(zh_text)\n",
        "        target_in = zh_id_vector[:,:-1].to_tensor()\n",
        "        target_out = zh_id_vector[:,1:].to_tensor()\n",
        "\n",
        "        return (en_padded, target_in), target_out"
      ],
      "metadata": {
        "id": "gECiyhqsewLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, text_processor, units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.text_processor = text_processor\n",
        "        self.vocab_size = text_processor.vocabulary_size()\n",
        "        self.units = units\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero=True)\n",
        "        self.rnn = tf.keras.layers.Bidirectional(merge_mode='sum',\n",
        "                        layer=tf.keras.layers.GRU(units,\n",
        "                                    # Return the sequence and state\n",
        "                                    return_sequences=True,\n",
        "                                    recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.rnn(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def convert_input(self, texts):\n",
        "        texts = tf.convert_to_tensor(texts)\n",
        "        if len(texts.shape) == 0:\n",
        "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "\n",
        "        context = self.text_processor(texts).to_tensor()\n",
        "        context = self(context)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "PWvjDm3VezrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, x, context):\n",
        "        attn_output, attn_scores = self.mha(\n",
        "            query=x,\n",
        "            value=context,\n",
        "            return_attention_scores=True)\n",
        "\n",
        "        # Cache the attention scores for plotting later.\n",
        "        attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "        self.last_attention_weights = attn_scores\n",
        "\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "cumuQkgje2Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, text_processor, units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.text_processor = text_processor\n",
        "        self.vocab_size = text_processor.vocabulary_size()\n",
        "        self.word_to_id = tf.keras.layers.StringLookup(\n",
        "            vocabulary=text_processor.get_vocabulary(),\n",
        "            mask_token='', oov_token='[UNK]')\n",
        "        self.id_to_word = tf.keras.layers.StringLookup(\n",
        "            vocabulary=text_processor.get_vocabulary(),\n",
        "            mask_token='', oov_token='[UNK]',\n",
        "            invert=True)\n",
        "        self.start_token = self.word_to_id(START_TOKEN)\n",
        "        self.end_token = self.word_to_id(END_TOKEN)\n",
        "\n",
        "        self.units = units\n",
        "\n",
        "        # 1. The embedding layer converts token IDs to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero=True)\n",
        "\n",
        "        # 2. The RNN keeps track of what's been generated so far.\n",
        "        self.rnn = tf.keras.layers.GRU(units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        # 3. The RNN output will be the query for the attention layer.\n",
        "        self.attention = CrossAttention(units)\n",
        "\n",
        "        # 4. This fully connected layer produces the logits for each\n",
        "        # output token.\n",
        "        self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "    def call(self, context, x, state=None, return_state=False):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        x, state = self.rnn(x, initial_state=state)\n",
        "\n",
        "        x = self.attention(x, context)\n",
        "        self.last_attention_weights = self.attention.last_attention_weights\n",
        "\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        if return_state:\n",
        "            return logits, state\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "    def tokens_to_text(self, tokens):\n",
        "        words = self.id_to_word(tokens)\n",
        "        result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "        result = tf.strings.regex_replace(result, '^ *<s> *', '')\n",
        "        result = tf.strings.regex_replace(result, ' *</s> *$', '')\n",
        "        return result"
      ],
      "metadata": {
        "id": "1agmvLjie5Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def call(self, inputs):\n",
        "        context, x = inputs\n",
        "        context = self.encoder(context)\n",
        "        logits = self.decoder(context, x)\n",
        "\n",
        "        try:\n",
        "            del logits._keras_mask\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def translate(self, input_sequence, max_length=50):\n",
        "        context = self.encoder.convert_input(input_sequence)\n",
        "        # context_vectors = self.encoder(context)\n",
        "\n",
        "        decoder_input = tf.expand_dims([self.decoder.start_token], 0)\n",
        "\n",
        "        output_tokens = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            logits = self.decoder(context, decoder_input)\n",
        "\n",
        "            next_token = tf.argmax(logits, axis=-1)\n",
        "\n",
        "            # Append next token to output tokens\n",
        "            output_tokens.append(next_token.numpy()[0, 0])  # Assuming batch size is 1\n",
        "\n",
        "            if next_token == self.decoder.end_token:\n",
        "                break\n",
        "\n",
        "            decoder_input = next_token\n",
        "\n",
        "        # Convert output tokens to words or sentences\n",
        "        translated_sequence = self.decoder.tokens_to_text(output_tokens)\n",
        "\n",
        "        return translated_sequence"
      ],
      "metadata": {
        "id": "FWhM3Xh0fLut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        super(TranslationCallback, self).__init__()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        source_text1 = \"I'm an industrial engineer.\"\n",
        "        result1 = model.translate([source_text1]) # 我是一个工业工程师\n",
        "\n",
        "        source_text2 = \"Hi, how are you today?\"\n",
        "        result2 = model.translate([source_text2])\n",
        "        print(result2.numpy().decode())\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} Translation:\")\n",
        "        print(f\"Source: {source_text1}\")\n",
        "        print(f\"Translated: {result1.numpy().decode()}\")\n",
        "        print(f\"Source: {source_text2}\")\n",
        "        print(f\"Translated: {result2.numpy().decode()}\")"
      ],
      "metadata": {
        "id": "gwZEDbjafNcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "###           ###\n",
        "###    Main   ###\n",
        "###           ###\n",
        "#################\n",
        "def masked_loss(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "\n",
        "def masked_acc(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "7qyoZRkLfQjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader()\n",
        "data_loader.build(\"/content/drive/MyDrive/[03] School/[01] NUS/[06] AY2324 Sem 2/CS4248 Natural Language Processing/[01] Project/Data/train.csv\", \"/content/drive/MyDrive/[03] School/[01] NUS/[06] AY2324 Sem 2/CS4248 Natural Language Processing/[01] Project/Data/validation.csv\")"
      ],
      "metadata": {
        "id": "62qcGst2hAxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UNITS = 256\n",
        "\n",
        "model = Translator(Encoder(data_loader.en_context_text_processor, UNITS), Decoder(data_loader.zh_context_text_processor, UNITS))\n",
        "model.compile(optimizer='adam', loss=masked_loss, metrics=[masked_acc, masked_loss])\n",
        "\n",
        "history = model.fit(\n",
        "    data_loader.train_ds.repeat(),\n",
        "    epochs=100,\n",
        "    steps_per_epoch = 100,\n",
        "    validation_data=data_loader.val_ds,\n",
        "    validation_steps = 20,\n",
        "    callbacks=[TranslationCallback()]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fkEeWb-ffUkY",
        "outputId": "9ec3c640-3bbb-4f83-bd85-ab13c49430fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.8577 - masked_acc: 0.0819 - masked_loss: 6.8577"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的\n",
            "\n",
            "Epoch 1 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 的 。\n",
            "Source: Hi, how are you today?\n",
            "Translated: 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的 ， 我 的\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 59s 448ms/step - loss: 6.8577 - masked_acc: 0.0819 - masked_loss: 6.8577 - val_loss: 6.4628 - val_masked_acc: 0.0971 - val_masked_loss: 6.4645\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.9992 - masked_acc: 0.1583 - masked_loss: 5.9992你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以\n",
            "\n",
            "Epoch 2 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 的 [UNK]\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以 你 可以\n",
            "100/100 [==============================] - 35s 344ms/step - loss: 5.9992 - masked_acc: 0.1583 - masked_loss: 5.9992\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.5817 - masked_acc: 0.2011 - masked_loss: 5.5817你 的 ？\n",
            "\n",
            "Epoch 3 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 的 。\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你 的 ？\n",
            "100/100 [==============================] - 29s 288ms/step - loss: 5.5817 - masked_acc: 0.2011 - masked_loss: 5.5817\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.3200 - masked_acc: 0.2247 - masked_loss: 5.3200好 ？\n",
            "\n",
            "Epoch 4 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 的\n",
            "Source: Hi, how are you today?\n",
            "Translated: 好 ？\n",
            "100/100 [==============================] - 28s 280ms/step - loss: 5.3200 - masked_acc: 0.2247 - masked_loss: 5.3200\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.1055 - masked_acc: 0.2443 - masked_loss: 5.1055“ 你 怎么 ？\n",
            "\n",
            "Epoch 5 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 的 [UNK]\n",
            "Source: Hi, how are you today?\n",
            "Translated: “ 你 怎么 ？\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 5.1055 - masked_acc: 0.2443 - masked_loss: 5.1055\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.9097 - masked_acc: 0.2616 - masked_loss: 4.9097你 怎么 ？\n",
            "\n",
            "Epoch 6 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 的 一个 系统\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你 怎么 ？\n",
            "100/100 [==============================] - 28s 278ms/step - loss: 4.9097 - masked_acc: 0.2616 - masked_loss: 4.9097\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.7557 - masked_acc: 0.2740 - masked_loss: 4.7557请 你们 怎么 ？\n",
            "\n",
            "Epoch 7 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 的 项目\n",
            "Source: Hi, how are you today?\n",
            "Translated: 请 你们 怎么 ？\n",
            "100/100 [==============================] - 26s 259ms/step - loss: 4.7557 - masked_acc: 0.2740 - masked_loss: 4.7557\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6134 - masked_acc: 0.2856 - masked_loss: 4.6134PM ： 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在\n",
            "\n",
            "Epoch 8 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 一个 项目\n",
            "Source: Hi, how are you today?\n",
            "Translated: PM ： 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在 你 现在\n",
            "100/100 [==============================] - 28s 283ms/step - loss: 4.6134 - masked_acc: 0.2856 - masked_loss: 4.6134\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.4977 - masked_acc: 0.2948 - masked_loss: 4.4977好 吗 ？\n",
            "\n",
            "Epoch 9 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 一个 工业\n",
            "Source: Hi, how are you today?\n",
            "Translated: 好 吗 ？\n",
            "100/100 [==============================] - 26s 263ms/step - loss: 4.4977 - masked_acc: 0.2948 - masked_loss: 4.4977\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.3861 - masked_acc: 0.3052 - masked_loss: 4.3861嗨 ？\n",
            "\n",
            "Epoch 10 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 一个 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 嗨 ？\n",
            "100/100 [==============================] - 25s 250ms/step - loss: 4.3861 - masked_acc: 0.3052 - masked_loss: 4.3861\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2900 - masked_acc: 0.3148 - masked_loss: 4.2900你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天\n",
            "\n",
            "Epoch 11 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 一个 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天\n",
            "100/100 [==============================] - 28s 279ms/step - loss: 4.2900 - masked_acc: 0.3148 - masked_loss: 4.2900\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1949 - masked_acc: 0.3206 - masked_loss: 4.1949你好 ？\n",
            "\n",
            "Epoch 12 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 一个 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你好 ？\n",
            "100/100 [==============================] - 26s 256ms/step - loss: 4.1949 - masked_acc: 0.3206 - masked_loss: 4.1949\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1334 - masked_acc: 0.3237 - masked_loss: 4.1334好 吗 ？\n",
            "\n",
            "Epoch 13 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 一个 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 好 吗 ？\n",
            "100/100 [==============================] - 25s 248ms/step - loss: 4.1334 - masked_acc: 0.3237 - masked_loss: 4.1334\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.0627 - masked_acc: 0.3287 - masked_loss: 4.0627你好 ？\n",
            "\n",
            "Epoch 14 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你好 ？\n",
            "100/100 [==============================] - 26s 256ms/step - loss: 4.0627 - masked_acc: 0.3287 - masked_loss: 4.0627\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9955 - masked_acc: 0.3346 - masked_loss: 3.9955你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天\n",
            "\n",
            "Epoch 15 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 一个 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天\n",
            "100/100 [==============================] - 27s 274ms/step - loss: 3.9955 - masked_acc: 0.3346 - masked_loss: 3.9955\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9409 - masked_acc: 0.3398 - masked_loss: 3.9409好 吧 ？\n",
            "\n",
            "Epoch 16 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 好 吧 ？\n",
            "100/100 [==============================] - 26s 255ms/step - loss: 3.9409 - masked_acc: 0.3398 - masked_loss: 3.9409\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8980 - masked_acc: 0.3436 - masked_loss: 3.8980嗨 ？\n",
            "\n",
            "Epoch 17 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 嗨 ？\n",
            "100/100 [==============================] - 26s 259ms/step - loss: 3.8980 - masked_acc: 0.3436 - masked_loss: 3.8980\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8586 - masked_acc: 0.3459 - masked_loss: 3.8586嗨 ？\n",
            "\n",
            "Epoch 18 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 嗨 ？\n",
            "100/100 [==============================] - 25s 247ms/step - loss: 3.8586 - masked_acc: 0.3459 - masked_loss: 3.8586\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5817 - masked_acc: 0.3662 - masked_loss: 3.5831大家 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天\n",
            "\n",
            "Epoch 19 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 大家 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天\n",
            "100/100 [==============================] - 28s 278ms/step - loss: 3.5817 - masked_acc: 0.3662 - masked_loss: 3.5831\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5259 - masked_acc: 0.3711 - masked_loss: 3.5259嗨 ？\n",
            "\n",
            "Epoch 20 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 嗨 ？\n",
            "100/100 [==============================] - 24s 243ms/step - loss: 3.5259 - masked_acc: 0.3711 - masked_loss: 3.5259\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5486 - masked_acc: 0.3685 - masked_loss: 3.5486嗨 ？\n",
            "\n",
            "Epoch 21 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 嗨 ？\n",
            "100/100 [==============================] - 25s 249ms/step - loss: 3.5486 - masked_acc: 0.3685 - masked_loss: 3.5486\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5193 - masked_acc: 0.3708 - masked_loss: 3.5193大家 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天\n",
            "\n",
            "Epoch 22 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 一个 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 大家 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天 你 今天\n",
            "100/100 [==============================] - 27s 275ms/step - loss: 3.5193 - masked_acc: 0.3708 - masked_loss: 3.5193\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5186 - masked_acc: 0.3721 - masked_loss: 3.5186你好 ？\n",
            "\n",
            "Epoch 23 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你好 ？\n",
            "100/100 [==============================] - 25s 249ms/step - loss: 3.5186 - masked_acc: 0.3721 - masked_loss: 3.5186\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4925 - masked_acc: 0.3732 - masked_loss: 3.4925你好 ？\n",
            "\n",
            "Epoch 24 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你好 ？\n",
            "100/100 [==============================] - 25s 255ms/step - loss: 3.4925 - masked_acc: 0.3732 - masked_loss: 3.4925\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4879 - masked_acc: 0.3738 - masked_loss: 3.4879你好 ？\n",
            "\n",
            "Epoch 25 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你好 ？\n",
            "100/100 [==============================] - 25s 246ms/step - loss: 3.4879 - masked_acc: 0.3738 - masked_loss: 3.4879\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4750 - masked_acc: 0.3751 - masked_loss: 3.4750你好 ？\n",
            "\n",
            "Epoch 26 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你好 ？\n",
            "100/100 [==============================] - 25s 253ms/step - loss: 3.4750 - masked_acc: 0.3751 - masked_loss: 3.4750\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4521 - masked_acc: 0.3772 - masked_loss: 3.4521你好 ？\n",
            "\n",
            "Epoch 27 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你好 ？\n",
            "100/100 [==============================] - 25s 247ms/step - loss: 3.4521 - masked_acc: 0.3772 - masked_loss: 3.4521\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4390 - masked_acc: 0.3785 - masked_loss: 3.4390嗨 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天\n",
            "\n",
            "Epoch 28 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 嗨 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天 你们 今天\n",
            "100/100 [==============================] - 27s 274ms/step - loss: 3.4390 - masked_acc: 0.3785 - masked_loss: 3.4390\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4460 - masked_acc: 0.3779 - masked_loss: 3.4460你好 ？\n",
            "\n",
            "Epoch 29 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你好 ？\n",
            "100/100 [==============================] - 25s 251ms/step - loss: 3.4460 - masked_acc: 0.3779 - masked_loss: 3.4460\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4034 - masked_acc: 0.3817 - masked_loss: 3.4034你好 ？\n",
            "\n",
            "Epoch 30 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 你好 ？\n",
            "100/100 [==============================] - 25s 247ms/step - loss: 3.4034 - masked_acc: 0.3817 - masked_loss: 3.4034\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.4115 - masked_acc: 0.3810 - masked_loss: 3.4115嗨 ？\n",
            "\n",
            "Epoch 31 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 嗨 ？\n",
            "100/100 [==============================] - 25s 246ms/step - loss: 3.4115 - masked_acc: 0.3810 - masked_loss: 3.4115\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3902 - masked_acc: 0.3821 - masked_loss: 3.3902嗨 ？\n",
            "\n",
            "Epoch 32 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师 。\n",
            "Source: Hi, how are you today?\n",
            "Translated: 嗨 ？\n",
            "100/100 [==============================] - 25s 247ms/step - loss: 3.3902 - masked_acc: 0.3821 - masked_loss: 3.3902\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3911 - masked_acc: 0.3834 - masked_loss: 3.3911今天 是 什么 ？\n",
            "\n",
            "Epoch 33 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师\n",
            "Source: Hi, how are you today?\n",
            "Translated: 今天 是 什么 ？\n",
            "100/100 [==============================] - 25s 247ms/step - loss: 3.3911 - masked_acc: 0.3834 - masked_loss: 3.3911\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.3818 - masked_acc: 0.3831 - masked_loss: 3.3818嗨 ？\n",
            "\n",
            "Epoch 34 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated: 我 是 工业 工程师 。\n",
            "Source: Hi, how are you today?\n",
            "Translated: 嗨 ？\n",
            "100/100 [==============================] - 24s 243ms/step - loss: 3.3818 - masked_acc: 0.3831 - masked_loss: 3.3818\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - ETA: 0s - loss: nan - masked_acc: 2.6097 - masked_loss: nan                                                 \n",
            "\n",
            "Epoch 35 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated:                                                  \n",
            "Source: Hi, how are you today?\n",
            "Translated:                                                  \n",
            "100/100 [==============================] - 32s 322ms/step - loss: nan - masked_acc: 2.6097 - masked_loss: nan\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - ETA: 0s - loss: nan - masked_acc: 2.8142 - masked_loss: nan                                                 \n",
            "\n",
            "Epoch 36 Translation:\n",
            "Source: I'm an industrial engineer.\n",
            "Translated:                                                  \n",
            "Source: Hi, how are you today?\n",
            "Translated:                                                  \n",
            "100/100 [==============================] - 29s 293ms/step - loss: nan - masked_acc: 2.8142 - masked_loss: nan\n",
            "Epoch 37/100\n",
            " 51/100 [==============>...............] - ETA: 11s - loss: nan - masked_acc: 2.9465 - masked_loss: nan"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4516bc35f430>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasked_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmasked_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1811\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1813\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1814\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;34m\"\"\"Updates the progbar.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_init_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# One-indexed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_maybe_init_progbar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;31m# first train step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             self.stateful_metrics = self.stateful_metrics.union(\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             )\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    917\u001b[0m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m_flatten_layers\u001b[0;34m(self, recursive, include_self)\u001b[0m\n\u001b[1;32m   3304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_flatten_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3306\u001b[0;31m         for m in self._flatten_modules(\n\u001b[0m\u001b[1;32m   3307\u001b[0m             \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3308\u001b[0m         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m_flatten_modules\u001b[0;34m(self, recursive, include_self)\u001b[0m\n\u001b[1;32m   3347\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0msubtrackables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3348\u001b[0m                             \u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextendleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtrackables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3349\u001b[0;31m                 elif isinstance(\n\u001b[0m\u001b[1;32m   3350\u001b[0m                     \u001b[0mtrackable_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3351\u001b[0m                     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackableDataStructure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}